\documentclass[11pt,aspectratio=169,handout]{beamer}

\usetheme{Singapore}
\usecolortheme{orchid}

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bibentry}
\usepackage{wasysym}
\usepackage[most]{tcolorbox}
\usepackage[normalem]{ulem}

\usepackage{hyperref}

\definecolor{info}{RGB}{62, 180, 137}
\definecolor{warn}{RGB}{128, 0, 0}

\author{Николай Анохин}
\title{Нерешенные проблемы и новые направления}

\titlegraphic{
   \includegraphics[height=0.07\textheight]{images/ok_logo.png}
   \includegraphics[height=0.07\textheight]{images/made_logo.png}
}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\begin{document}

{
\setbeamertemplate{headline}{}

\begin{frame}
\titlepage
\end{frame}

%\begin{frame}
%\tableofcontents
%\end{frame}

}

\begin{frame}{Контекст}

\begin{center}
\includegraphics[scale=0.23]{images/mendeley.jpeg}
\end{center}

\end{frame}

\begin{frame}{Что мы уже умеем}

\begin{columns}

\begin{column}{0.2\textwidth}

\end{column}

\begin{column}{0.5\textwidth} 

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
\begin{Large}
\[
\hat r_{ui} = f_{\theta}(x_u, x_i, x_c)
\]
\end{Large}
\end{tcolorbox}

\end{column}

\begin{column}{0.2\textwidth} 

\end{column}
\end{columns}

\vfill

Проблемы
\begin{enumerate}[<+->]
\item Оцениваем айтемы по-отдельности, а показываем по несколько (лентой)
\item Модель не объясняет, почему именно эти айтемы подходят пользователю
\item Смещение между распределениями на обучении и применении
\item Не учитывается долгострочный эффект рекомендаций
\end{enumerate}

\end{frame}

\section{Разнообразие в рекомендательных системах}

\begin{frame}{Разнообразие / Diversity}

\begin{center}
\includegraphics[scale=0.22]{images/diversity.png}
\end{center}

\end{frame}

\begin{frame}{Набираем айтемы с разными аспектами}

$f$ - аспект (признак) айтема, $p(f | i)$ -- вероятность найти аспект у айтема $i$

\vfill

Распределение аспекта у пользователя

\[
p(f | u) = \frac{\sum_{i \in I_u} p(f | i)}{|I_u|}  
\]

Распределение аспекта в рекомендациях
\[
q(f | u) = \frac{\sum_{i \in RL} p(f | i)}{|RL|}
\]

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Формируем список так, чтобы $q(f | u)$ совпало с $p(f | u)$
\end{tcolorbox}

\end{frame}

\begin{frame}{Жадное переранжирование}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Добавляем в список рекомендаций айтем с максимальным значением
\[
(1 - \lambda) \cdot s(u, i) + \lambda \cdot gain(i, RL),
\]
пока не получим список нужной длины.
\end{tcolorbox}

\vfill

\begin{itemize}
\item $s(u, i)$ -- релевантность айтема $i$ для пользователя $u$ 
\item $gain(i, RL) = div(RL \cup \{i\}) - div(RL)$ -- улучшение разнообразия при добавлении айтема
\item $\lambda$ -- гиперпараметр
\end{itemize}

\end{frame}

\begin{frame}{A Comparison of Calibrated and Intent-Aware Recommendations \cite{BRIDGE}}
\begin{center}
\includegraphics[scale=0.5]{images/spad.png}
\end{center}
\end{frame}

\begin{frame}{Учим разнообразие вместе с моделью 1}

\begin{center}
Controllable Multi-Interest Framework for Recommendation \cite{ALIBABA}
\end{center}

\begin{columns}
\begin{column}{0.45\textwidth}
\begin{center}
\includegraphics[scale=0.3]{images/alibaba.png}
\end{center}
\end{column}

\begin{column}{0.45\textwidth} 
\begin{center}
\includegraphics[scale=0.2]{images/divres.png}
\end{center}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Учим разнообразие вместе с моделью 2}

\begin{center}
Managing Diversity in Airbnb Search \cite{AIRBNB}
\end{center}

\begin{columns}

\begin{column}{0.45\textwidth}
\begin{center}
\includegraphics[scale=0.25]{images/airbnb2.png}
\end{center}
\end{column}

\begin{column}{0.45\textwidth} 
\begin{center}
\includegraphics[scale=0.25]{images/airbnb.png}
\end{center}
\end{column}

\end{columns}

\end{frame}

\begin{frame}{}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Из-за поточечного предсказания релевантности, приходится дополнительно разнообразить списки рекомендаций
\end{tcolorbox}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Необходимость разнообразия обосновывается A/B экспериментом
\end{tcolorbox}

\end{frame}

\section{Объяснение рекомендаций}

\begin{frame}{Объяснения}

\begin{center}
\includegraphics[scale=0.22]{images/explainability-1.png}
\end{center}

\end{frame}

\begin{frame}{Объяснения}

\begin{center}
\includegraphics[scale=0.22]{images/explainability-2.png}
\end{center}

\end{frame}

\begin{frame}{Зачем объяснять рекомендации?}

\begin{enumerate}[<+->]
\item Прозрачность: объяснить пользователю, как работает система
\item Контролируемость: позволить пользователю исправить ошибки
\item Доверие: убедить пользователя, что система работает правильно
\item Убеждение: мотивировать пользователя к покупке
\item Полезность: помочь пользователю принять правильное решение
\item Эффективность: помочь пользователю принять решение быстро
\item Удовольствие: сделать приятно пользователю
\end{enumerate}

\end{frame}

\begin{frame}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=Case-based]
Because you have selected or highly rated: Movie A
\end{tcolorbox}

\vfill

\begin{center}
\includegraphics[scale=0.3]{images/netflix.png}
\end{center}

\end{frame}

\begin{frame}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=Collaborative]
Customers Who Bought This Item Also Bought A
\end{tcolorbox}

\vfill

\begin{center}
\includegraphics[scale=0.3]{images/amazon.jpeg}
\end{center}

\end{frame}

\begin{frame}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=Content-based]
Recommended because you said liked science fiction
\end{tcolorbox}

\vfill

\begin{center}
\includegraphics[scale=0.3]{images/facebook.png}
\end{center}

\end{frame}

\begin{frame}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=Knowledge-based]
Less Memory and Lower Resolution and Cheaper
\end{tcolorbox}

\vfill

\begin{center}
\includegraphics[scale=0.2]{images/amazon2.png}
\end{center}

\end{frame}

\begin{frame}{Explore, Exploit, and Explain: \\ Personalizing Explainable Recommendations with Bandits \cite{EX3}}

\begin{columns}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[scale=0.4]{images/spotify.png}
\end{center}
\end{column}

\begin{column}{0.5\textwidth}
\[
r(j, e, x) = \sigma(\theta_{global} + \theta_{j} \times 1_j + \theta_e \times 1_e + \theta_x \times 1_x)
\]
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Why I like it: Multi-task Learning for Recommendation and Explanation \cite{GAN}}

\begin{center}
\includegraphics[scale=0.4]{images/gan.png}
\end{center}

\end{frame}

\begin{frame}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Если хотим делать объяснения рекомендаций, нужно ответить на вопросы:
\begin{itemize}
\item Какую цель мы достигнем объяснениями?
\item Какие объяснения можно получить из модели?
\item Как правильно представить объяснения пользователю?
\end{itemize}
\end{tcolorbox}

\end{frame}

\section{Смещения}

\begin{frame}{Удачные рекомендации}

\begin{center}
\includegraphics[scale=0.22]{images/serendipity-pony.png}
\end{center}

\end{frame}

\begin{frame}{Пример self-selection bias}

\begin{center}
\includegraphics[scale=0.3]{images/bias-example.png}
\end{center}

\[
R(\hat Y) = \frac{1}{U I} \sum_u \sum_i \delta_{ui} (Y, \hat Y), \quad R_{naive}(\hat Y) = \frac{1}{N} \sum_{(u,i) \in D} \delta_{ui}(Y, \hat Y)
\]

\end{frame}

\begin{frame}{Смещения в рекомендациях \cite{BIAS}}

\begin{center}
\includegraphics[scale=0.2]{images/bias.png}
\end{center}

\end{frame}

\begin{frame}{Inverse Propensity Scored Estimator \cite{TREATMENTS}}

$P_{ui} = P((u, i) \in D)$ -- вероятность, что пользователь $u$ поставит оценку айтему $i$

\[
R_{IPS}(\hat Y | P) = \frac{1}{U I} \sum_{(u,i) \in D} \frac{\delta_{ui}(Y, \hat Y)}{P_{ui}}
\]

\[
E_D [R_{IPS}(\hat Y | P)] = \frac{1}{U I} \sum_u \sum_i E_D\left[ \frac{\delta_{ui}(Y, \hat Y)}{P_{ui}} \mathbb{I}\{(u, i) \in D\}\right] = 
\]
\[
= \frac{1}{U I} \sum_u \sum_i \delta_{ui} (Y, \hat Y) = R(\hat Y) 
\]

\end{frame}

\begin{frame}{IPS Estimator: проблемы}

\begin{enumerate}
\item Когда $P_{ui}$ неизвестно, его приходится оценивать
\item Большая дисперсия при оценке $P_{ui}$
\item Непонятно, как быть с рекомендациями списков
\end{enumerate}

\end{frame}

\begin{frame}{Causal рекомендации}

\begin{tcolorbox}[colback=warn!5,colframe=warn!80,title=Традиционный рекомендер]
Посмотрит ли пользователь этот фильм, если известно что она смотрела в прошлом?
\end{tcolorbox}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=Causal рекомендер]
Посмотрит ли пользователь этот фильм, если мы его порекомендуем, и известно, что она смотрела в прошлом?
\end{tcolorbox}

\end{frame}

\begin{frame}{The Deconfounded Recommender \cite{DECONF}}

\vfill

\begin{tcolorbox}[colback=gray!5,colframe=gray!80,title=Confounder]
Переменная, которая влияет и на treatment assignment, и на outcome
\end{tcolorbox}

\begin{columns}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[scale=0.2]{images/blessings.png}
\end{center}
\end{column}

\begin{column}{0.5\textwidth}
\begin{enumerate}
\item Учим модель $p(z, a_1, \ldots, a_m)$
\item Оцениваем $E(z_j | a_{1j}, \ldots, a_{mj})$ для каждого наблюдения
\item Используем оценки для $z_j$ как признак в рекомендере
\end{enumerate}
\end{column}
\end{columns}

\end{frame}

\begin{frame}

\begin{center}
\includegraphics[scale=0.6]{images/deconf-result.png}
\end{center}

\end{frame}

\begin{frame}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Из-за специфики сбора данных рекомендации подвержены смещениям. 
\end{tcolorbox}
\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Существуют техники для корректировки, но они несовершенны.
\end{tcolorbox}

\end{frame}

\section{Долгосрочный эффект рекомендаций}

\begin{frame}{}

\begin{center}
\includegraphics[scale=0.22]{images/longterm.png}
\end{center}

\end{frame}

\begin{frame}{Долгосрочный эффект рекомендаций}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
\begin{enumerate}[<+->]
\item Эволюция пользователя (рекомендер влияет на пользователя)
\item Эволюция рекомендера (рекомендер влияет на себя)
\item Отложенная награда
\end{enumerate}
\end{tcolorbox}

\end{frame}

\begin{frame}{Рекомендации как Reinforcement Learning}

\begin{columns}

\begin{column}{0.3\textwidth}
\begin{center}
\includegraphics[scale=0.2]{images/rl.jpeg}
\end{center}
\end{column}

\begin{column}{0.65\textwidth}

\begin{footnotesize}
\begin{center}
\begin{tabular}{l c l}
RecSys & $\rightarrow$ & RL \\
\hline
Пользователь & $\rightarrow$ & \pause Среда (environment) \\
Контекст & $\rightarrow$ & \pause Наблюдение (observation) \\
Рекомендательный сервис & $\rightarrow$ &  \pause Агент (agent) \\
Алгоритм рекомендаций & $\rightarrow$ &  \pause Политика (policy) \\
Рекомендация & $\rightarrow$ & \pause Действие (action) \\ 
Покупка, просмотр, клик & $\rightarrow$ &  \pause Награда (reward) \\
??? & $\rightarrow$ &  \pause Эпизод (episode) \\
\end{tabular}
\end{center}
\end{footnotesize}

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Почему RL (почти) не используется в продакшен рекомендерах?}

\begin{tcolorbox}[colback=warn!5,colframe=warn!80,title=]
\begin{itemize}[<+->]
\item Огромное меняющееся пространство действий-состояний
\item Отсутствие данных (сред) для проверки идей
\item Дорогая реализация алгоритмов
\end{itemize}
\end{tcolorbox}

\end{frame}

\section{Многорукие бандиты}

\begin{frame}{Multi-armed bandit}

\begin{center}
\includegraphics[scale=0.3]{images/mab.png}
\end{center}

\[
Q_n(a) = \rm I\!E[R_n \mid A_n = a]
\]
\[
A^*_n = \max_a Q_n(a)
\]

\end{frame}

\begin{frame}{Варианты решений I \cite{BANDITS1}}

\begin{itemize}
\item $\varepsilon$-greedy: выбираем случайную руку с вероятностью $\varepsilon$, иначе жадно
\item $\varepsilon$-decay: как $\varepsilon$-greedy, но уменьшаем $\varepsilon$ со временем
\[
\varepsilon(n) = \frac{1}{1 + n \beta}
\]
\item Upper Confidence Bound (UCB)
\[
A_n = \arg \max_a \left( Q_n(a) + c \sqrt{\frac{\log(n)}{N_n(a)}} \right)
\]
\end{itemize}

\end{frame}

\begin{frame}{Варианты решений II: Gradient Bandit \cite{BANDITS2}}

Политика, которая чаще выбирает "хорошие" руки

$H(A_k)$ -- value руки $k$
\[
\pi(A_k) = \frac{\exp H(A_k)}{\sum_j \exp H(A_j)}
\]
Обновление
\[
H_{t+1} (A_t) = H_t(A_t) + \alpha (R_t - \bar R_t)(1 - \pi_t(A_t))
\]
\[
H_{t+1} (a) = H_t(a) - \alpha (R_t - \bar R_t)\pi_t(a), \; \forall a \neq A_t
\]

\end{frame}

\begin{frame}{Варианты решений III: Thompson Sampling}

\begin{columns}

\begin{column}{0.4\textwidth}
\begin{enumerate}
\item Для каждой руки оцениваем распределение награды
\item Семплируем значение из каждого из распределений
\item Выбираем руку с наибольшим значением
\end{enumerate}
\end{column}

\begin{column}{0.55\textwidth}

\begin{small}
\begin{center}
\includegraphics[scale=0.2]{images/thompson.png}
\end{center}
\end{small}

\end{column}
\end{columns}

\end{frame}

\begin{frame}{Сравнение алгоритмов \cite{BANDITS3}}

\begin{center}
\includegraphics[scale=0.2]{images/regret.png}
\end{center}
\[
Regret_t = G_t^{optimal} - G_t
\]

\end{frame}

\begin{frame}{RecSim: A Configurable Simulation Platform for Recommender Systems \cite{RECSIM}}

\begin{center}
\includegraphics[scale=0.2]{images/recsim.png}
\end{center}

\end{frame}

\begin{frame}{Deep Reinforcement Learning in Large Discrete Action Spaces \cite{DDPG}\footnote{Пример использования в рекомендациях: \url{https://arxiv.org/abs/1811.05869}}}

\begin{columns}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[scale=0.25]{images/ddpg.png}
\end{center}
\end{column}

\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[scale=0.25]{images/ddpg-result.png}
\end{center}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Top-K Off-Policy Correction for a REINFORCE Recommender System
 \cite{TOPK}}
 
\begin{itemize}
\item Масштабировали алгоритм REINFORCE на огромное пространство действий.
\item Применили корректировкуу смещения между logging и обучаемой политикой.
\item Изобрели новую корректировку на top-k рекомендации.
\item Применили все это в продакшене YouTube.
\end{itemize}

\begin{center}
\includegraphics[scale=0.25]{images/topk.png}
\end{center}

\end{frame}

\begin{frame}{Итоги}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Постановка задачи RL очень хорошо соответствует задаче рекомендаций.
\end{tcolorbox}

\vfill

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
В рекомендациях все признают проблемы explore/exploit и смещений. Их решают методами, заимствованными из RL.
\end{tcolorbox}

\vfill

\begin{tcolorbox}[colback=warn!5,colframe=warn!80,title=]
Придется подождать, пока RL в рекомендациях станет общей практикой.
\end{tcolorbox}

\end{frame}

\section{Итоги}

\begin{frame}{Итоги}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
При построении моделей мы делаем упрощающие предположения. Из-за этих предположений в продакшен системах могут возникать негативные эффекты. Эти эффекты нужно учитывать и пытаться исправить.
\end{tcolorbox}

\end{frame}

\begin{frame}{Итоги курса}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
В будущем рекомендательные системы будут давать релевантные, разнообразные и полезные рекомендации. Они будут учитывать долгосрочные интересы пользователей. А пользователи будут понимать, почему им что-то предлагают и смогут котролировать механизмы построения рекомендаций.
\end{tcolorbox}

\begin{tcolorbox}[colback=info!5,colframe=info!80,title=]
Но понадобится ваша помощь. И научная честность.
\end{tcolorbox}

\end{frame}

\begin{frame}{Входной опрос}

\begin{center}
\includegraphics[scale=0.4]{images/poll.png}
\end{center}

\end{frame}

\begin{frame}{}

\begin{center}
\includegraphics[scale=0.4]{images/thankyou.jpeg}
\end{center}

\end{frame}

\begin{frame}[allowframebreaks]{Литература}

\bibliographystyle{amsalpha}
\bibliography{references.bib}

\end{frame}

\end{document}
